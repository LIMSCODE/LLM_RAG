{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ba7b247",
      "metadata": {
        "id": "6ba7b247"
      },
      "source": [
        "# Step 1: Instruction Fine-Tuning GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74f0e7d",
      "metadata": {
        "id": "c74f0e7d"
      },
      "outputs": [],
      "source": [
        "!pip install datasets #허깅페이스\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, pipeline, \\\n",
        "                         AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "import torch\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7463affd",
      "metadata": {
        "id": "7463affd"
      },
      "outputs": [],
      "source": [
        "seed=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3f6ef34",
      "metadata": {
        "id": "d3f6ef34"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75ea597",
      "metadata": {
        "id": "b75ea597"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "data_path = '/content/drive/MyDrive/fine tuning/chip2.csv'\n",
        "training_df = pd.read_csv(data_path)                #####csv를 데이터프레임으로 변환\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9161cfe5",
      "metadata": {
        "id": "9161cfe5"
      },
      "source": [
        "![Screenshot%202023-08-02%20at%206.48.20%20AM.png](attachment:Screenshot%202023-08-02%20at%206.48.20%20AM.png)\n",
        "\n",
        "Find it [here](https://laion.ai/blog/oig-dataset/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6ba917",
      "metadata": {
        "id": "cb6ba917"
      },
      "outputs": [],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cbd19f",
      "metadata": {
        "scrolled": true,
        "id": "d3cbd19f"
      },
      "outputs": [],
      "source": [
        "print(training_df.iloc[798]['prompt'])            ##### iloc는 798은 데이터프레임의 799번째 행\n",
        "print(\"========\")\n",
        "print(training_df.iloc[798]['response'])\n",
        "print(\"========\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98375f2",
      "metadata": {
        "id": "f98375f2"
      },
      "outputs": [],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c9f918",
      "metadata": {
        "id": "37c9f918"
      },
      "outputs": [],
      "source": [
        "training_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9265bf12",
      "metadata": {
        "id": "9265bf12"
      },
      "outputs": [],
      "source": [
        "training_df['response'] = training_df['response'].map(lambda x: x.strip())   ##### map은 각요소에 함수적용 , 익명함수 람다, 각요소x에대해 strip()공백제거\n",
        "training_df['prompt'] = training_df['prompt'].map(lambda x: x.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "286c46cf",
      "metadata": {
        "id": "286c46cf"
      },
      "outputs": [],
      "source": [
        "training_df = training_df.drop_duplicates(subset=['prompt'])                 ##### prompt 열을 기준으로 중복을 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938d7c1e",
      "metadata": {
        "id": "938d7c1e"
      },
      "outputs": [],
      "source": [
        "training_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c02af40",
      "metadata": {
        "scrolled": true,
        "id": "9c02af40"
      },
      "outputs": [],
      "source": [
        "training_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1213393",
      "metadata": {
        "id": "b1213393"
      },
      "outputs": [],
      "source": [
        "training_df['source'].value_counts().plot(kind='bar')                      ##### 종류별분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caee1bb1",
      "metadata": {
        "id": "caee1bb1"
      },
      "outputs": [],
      "source": [
        "training_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d355929",
      "metadata": {
        "id": "7d355929"
      },
      "outputs": [],
      "source": [
        "training_df['prompt'].str.len().plot(kind='hist', title='Histogram of prompt Length')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5f1ed9",
      "metadata": {
        "id": "5e5f1ed9"
      },
      "outputs": [],
      "source": [
        "training_df['response'].str.len().plot(kind='hist', title='Histogram of response Length')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  \n",
        "# 모델학습 시작\n"
      ],
      "metadata": {
        "id": "8Y1ZTXrrMctn"
      },
      "id": "8Y1ZTXrrMctn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29058109",
      "metadata": {
        "id": "29058109"
      },
      "outputs": [],
      "source": [
        "QUERY_KEY = \"Question:\"\n",
        "RESPONSE_KEY = \"Response:\"\n",
        "PAD_KEY = \"<PAD>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecc72e1",
      "metadata": {
        "scrolled": true,
        "id": "4ecc72e1"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL = 'gpt2'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "# 사용자토큰정의\n",
        "# 일반적인 특수 토큰(e.g., <pad>, <cls>) 외에 추가로 사용자 정의 특수 토큰을 포함\n",
        "# 질문,답변을 구분 / 예:\"What is AI?\"를 \"<QUERY> What is AI?\"로 변환. / 입력 시퀀스의 길이를 맞추기 위해 패딩 토큰을 사용\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [QUERY_KEY, RESPONSE_KEY, PAD_KEY]})\n",
        "tokenizer.pad_token = PAD_KEY\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
        "model.config.pad_token = PAD_KEY\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34df23c0",
      "metadata": {
        "scrolled": true,
        "id": "34df23c0"
      },
      "outputs": [],
      "source": [
        "chip2_dataset = Dataset.from_pandas(training_df)                ##### 허깅페이스 Dataset라이브러리로 pandas 데이터프레임을 처리\n",
        "\n",
        "# 사용자토큰 적용함수\n",
        "def preprocess(example):\n",
        "    return tokenizer(QUERY_KEY+ ' ' + example['prompt'] + '\\n'+RESPONSE_KEY+' '+example['response']+tokenizer.eos_token)\n",
        "\n",
        "chip2_dataset = chip2_dataset.map(\n",
        "    preprocess, batched=False, batch_size=1024, remove_columns=chip2_dataset.features.keys()\n",
        ")\n",
        "\n",
        "# 길이조정함수\n",
        "def filter_function(example):\n",
        "    return len(example['input_ids']) <= 1024\n",
        "\n",
        "chip2_dataset = chip2_dataset.filter(filter_function)\n",
        "\n",
        "chip2_dataset = chip2_dataset.train_test_split(test_size=0.2, seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b831d29",
      "metadata": {
        "id": "0b831d29"
      },
      "outputs": [],
      "source": [
        "chip2_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d44c01d3",
      "metadata": {
        "id": "d44c01d3"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(chip2_dataset['test']['input_ids'][13]))\n",
        "print('-----')\n",
        "print(tokenizer.decode(chip2_dataset['test']['input_ids'][14]))\n",
        "print('-----')\n",
        "print(tokenizer.decode(chip2_dataset['test']['input_ids'][63]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b3462e",
      "metadata": {
        "id": "e7b3462e"
      },
      "outputs": [],
      "source": [
        "len(chip2_dataset['test']['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5050168a",
      "metadata": {
        "id": "5050168a"
      },
      "outputs": [],
      "source": [
        "f'{model.num_parameters():,}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a613d3",
      "metadata": {
        "id": "f8a613d3"
      },
      "outputs": [],
      "source": [
        "import wandb                  #### 모델학습 추적, 시각화\n",
        "# Set up Weights and Biases integration\n",
        "wandb.init(project=\"FFT\") #b93a105e31d833b072c1f15f57a095f50c62e0d3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "365f7b31",
      "metadata": {
        "id": "365f7b31"
      },
      "outputs": [],
      "source": [
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "print(torch.cuda.get_device_name(0))\n",
        "if device.type == 'cuda':\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5244b417",
      "metadata": {
        "id": "5244b417"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        "    return_tensors=\"pt\", pad_to_multiple_of=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2813eb1",
      "metadata": {
        "id": "d2813eb1"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "##### 실험설정 다르게 2번돌림\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fineframe_supervised_instruction\",  # The output directory\n",
        "    overwrite_output_dir=True,  # overwrite the content of the output directory\n",
        "    num_train_epochs=1,  # number of training epochs\n",
        "    per_device_train_batch_size=2,  # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    gradient_accumulation_steps=16,  # steps for gradient accumulation\n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    report_to=\"all\",\n",
        "    seed=seed,\n",
        "    fp16=True,  # enable mixed precision training for my GPU\n",
        ")\n",
        "\n",
        "# GPT2모델을 chip2데이터셋으로 훈련시킨다 (fine tuning)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=chip2_dataset['train'],\n",
        "    eval_dataset=chip2_dataset['test'],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b8b5abb",
      "metadata": {
        "scrolled": true,
        "id": "9b8b5abb"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fineframe\",  # The output directory\n",
        "    overwrite_output_dir=True,  # overwrite the content of the output directory\n",
        "    num_train_epochs=1,  # number of training epochs\n",
        "    per_device_train_batch_size=2,  # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    gradient_accumulation_steps=16,  # steps for gradient accumulation\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    report_to=\"all\",\n",
        "    seed=seed,\n",
        "    fp16=True,  # enable mixed precision training for GPU\n",
        ")\n",
        "\n",
        "##### transformers라이브러리 Trainer에 모델, args, 데이터셋 넣고 train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=chip2_dataset['train'],\n",
        "    eval_dataset=chip2_dataset['test'],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69ce1b90",
      "metadata": {
        "id": "69ce1b90"
      },
      "outputs": [],
      "source": [
        "number_of_examples = len(chip2_dataset['train'])\n",
        "per_device_train_batch_size = training_args.per_device_train_batch_size\n",
        "gradient_accumulation_steps = training_args.gradient_accumulation_steps\n",
        "num_train_epochs = training_args.num_train_epochs\n",
        "\n",
        "effective_batch_size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "number_of_training_steps = (number_of_examples / effective_batch_size) * num_train_epochs\n",
        "\n",
        "number_of_training_steps = int(number_of_training_steps / training_args.n_gpu)\n",
        "\n",
        "print(\"Number of training steps:\", number_of_training_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d98da89a",
      "metadata": {
        "id": "d98da89a"
      },
      "outputs": [],
      "source": [
        "training_args.warmup_steps = int(.1 * number_of_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707bc327",
      "metadata": {
        "scrolled": false,
        "id": "707bc327"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0457ee8c",
      "metadata": {
        "id": "0457ee8c"
      },
      "outputs": [],
      "source": [
        "# 모델 저장\n",
        "model_save_path = \"/content/drive/My Drive/fine tuning/trained_model\"\n",
        "tokenizer_save_path = \"/content/drive/My Drive/fine tuning/trained_model/tokenizer\"\n",
        "\n",
        "trainer.save_model(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dea18d2",
      "metadata": {
        "id": "4dea18d2"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(tokenizer_save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델을 구글드라이브에 저장된것을 불러옴**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Plln1wvSYGpi"
      },
      "id": "Plln1wvSYGpi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f2e331",
      "metadata": {
        "id": "03f2e331"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# 저장된 경로\n",
        "model_load_path = \"/content/drive/My Drive/fine tuning/trained_model\"\n",
        "tokenizer_load_path = \"/content/drive/My Drive/fine tuning/trained_model/tokenizer\"\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "model = AutoModelForCausalLM.from_pretrained(model_load_path)    # 처음엔 model = AutoModelForCausalLM.from_pretrained(\"GPT2\")\n",
        "# 모델사용 목적이다름, CausalLM 모델은 언어 생성에 필요한 자동 회귀 방식으로 학습됩니다.\n",
        "# Sequence Classification 모델은 분류 작업을 위한 엔드 투 엔드 분류 구조로 학습됩니다. AutoModelForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_load_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "650d8f2e",
      "metadata": {
        "id": "650d8f2e"
      },
      "source": [
        "### Loss drops dramatically which is expected when we introduce new tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97cddc97",
      "metadata": {
        "id": "97cddc97"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d019ce",
      "metadata": {
        "id": "42d019ce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}